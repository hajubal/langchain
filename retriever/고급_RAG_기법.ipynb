{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "_HBhuL23S_YR",
    "digNJMyRtM3k",
    "AK752NU_tHjb",
    "FoTL9eBctWva",
    "zQP9iztA8ApB"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb openai"
   ],
   "metadata": {
    "id": "GiQs8-_Nn_Ml",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:33:27.012502Z",
     "start_time": "2024-08-07T12:33:12.725814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 2.20.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Query Retriever"
   ],
   "metadata": {
    "id": "_HBhuL23S_YR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://n.news.naver.com/mnews/article/003/0012317114?sid=105\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ],
   "metadata": {
    "id": "Y3TWw12cSqRw",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:33:51.449838Z",
     "start_time": "2024-08-07T12:33:27.015148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/activations_tf.py:22\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtf_keras\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m, \u001B[38;5;167;01mImportError\u001B[39;00m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1567\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1204\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:38\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001B[0;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations_tf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_tf_activation\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfiguration_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PretrainedConfig\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/activations_tf.py:27\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m parse(keras\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mmajor \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     28\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     29\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     30\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install tf-keras`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m         )\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_gelu\u001B[39m(x):\n",
      "\u001B[0;31mValueError\u001B[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1567\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1204\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:35\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel, TFPreTrainedModel\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m version\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1229\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[0;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1557\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1557\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[1;32m   1558\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1569\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1570\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1571\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1572\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjhgan/ko-sbert-nli\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     17\u001B[0m encode_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnormalize_embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m}\n\u001B[0;32m---> 18\u001B[0m ko_embedding \u001B[38;5;241m=\u001B[39m HuggingFaceEmbeddings(\n\u001B[1;32m     19\u001B[0m     model_name\u001B[38;5;241m=\u001B[39mmodel_name,\n\u001B[1;32m     20\u001B[0m     encode_kwargs\u001B[38;5;241m=\u001B[39mencode_kwargs\n\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     23\u001B[0m vectordb \u001B[38;5;241m=\u001B[39m Chroma\u001B[38;5;241m.\u001B[39mfrom_documents(documents\u001B[38;5;241m=\u001B[39msplits, embedding\u001B[38;5;241m=\u001B[39mko_embedding)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:59\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import sentence_transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install sentence-transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/__init__.py:7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[1;32m      3\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCrossEncoder\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:18\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceEvaluator\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputExample\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceTransformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fullname, get_device_name, import_from_string\n\u001B[1;32m     21\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trange\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_torch_npu_available\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_card\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msimilarity_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SimilarityFunction\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/sentence_transformers/model_card.py:23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainerCallback\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CodeCarbonCallback\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodelcard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_markdown_table\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer_callback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainerControl, TrainerState\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1229\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[0;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1557\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1555\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1557\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[1;32m   1558\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonEdu/lib/python3.11/site-packages/transformers/utils/import_utils.py:1569\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1570\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1571\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1572\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"삼성전자 갤럭시 S24는 어떨 예정이야?\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"sk-\")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ],
   "metadata": {
    "id": "epoTy2FNSuqz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ],
   "metadata": {
    "id": "eU6X_HctSumR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ],
   "metadata": {
    "id": "pXYmox2MSuZ-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_docs"
   ],
   "metadata": {
    "id": "MIz2l4OaS75w"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 기본 Parent-document Retriever"
   ],
   "metadata": {
    "id": "digNJMyRtM3k"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tZOk-3LOnb-0",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:43:47.197643Z",
     "start_time": "2024-08-07T12:43:46.226861Z"
    }
   },
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ],
   "metadata": {
    "id": "f0ONC3wZn62e",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:43:43.595739Z",
     "start_time": "2024-08-07T12:43:43.585598Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[복지이슈 FOCUS 15ȣ] 경기도 극저신용대출심사모형 개발을 위한 국내 신용정보 활용가능성 탐색.pdf\"),\n",
    "    PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())"
   ],
   "metadata": {
    "id": "VpJhZWThn7gG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:33.148690Z",
     "start_time": "2024-08-07T12:40:00.690738Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tf-keras",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\r\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting tensorflow<2.18,>=2.17 (from tf-keras)\r\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (23.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.25.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.31.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (68.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.62.1)\r\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras)\r\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.2.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.36.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.41.2)\r\n",
      "Requirement already satisfied: rich in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.7.1)\r\n",
      "Requirement already satisfied: namex in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.7.4)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/hajubal/anaconda3/envs/pythonEdu/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\r\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m10.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl (236.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m236.2/236.2 MB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tensorboard, tensorflow, tf-keras\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.16.2\r\n",
      "    Uninstalling tensorboard-2.16.2:\r\n",
      "      Successfully uninstalled tensorboard-2.16.2\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.16.1\r\n",
      "    Uninstalling tensorflow-2.16.1:\r\n",
      "      Successfully uninstalled tensorflow-2.16.1\r\n",
      "Successfully installed tensorboard-2.17.0 tensorflow-2.17.0 tf-keras-2.17.0\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "KBb-1ym5pKRG",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:41:44.959987Z",
     "start_time": "2024-08-07T12:40:55.013168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5805b4c3df1e4029a5160fd5b5b9d34c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd46ebb2b5d348a784d542f4c6d3c966"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/4.46k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e36e2496dd0742438c134029aa68a21d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6241c675b50a48b5b7ce2a4a896dc844"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dba44a831c944a6d8924f81cd07e49b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbf4120dc04b40e58ed9caeb75e775d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66aa653942194e42b24c9a80d12f0eb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c65d47a78d694b63a37471e4be36eeb2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85a22bcabc47495482a0960d1832827d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61520b13f8994691a2e946d86480aa8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cc4c99f6341437a97d3320c53e24987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ],
   "metadata": {
    "id": "fyixQa5SpHOW",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:43:50.095288Z",
     "start_time": "2024-08-07T12:43:50.074245Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ],
   "metadata": {
    "id": "IyVXzfq0pHME",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:43:59.200595Z",
     "start_time": "2024-08-07T12:43:55.647322Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "sub_docs = vectorstore.similarity_search(\"인공지능 예산\")"
   ],
   "metadata": {
    "id": "L8z2B2rKrAv3",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:44:34.662849Z",
     "start_time": "2024-08-07T12:44:31.627391Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[0].page_content)"
   ],
   "metadata": {
    "id": "_axFoMvnrA-h",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:48:13.661404Z",
     "start_time": "2024-08-07T12:48:13.656819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글 길이: 66\n",
      "\n",
      "\n",
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"인공지능 예산\")"
   ],
   "metadata": {
    "id": "W68G7ITorThj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ],
   "metadata": {
    "id": "Uzl334J8rXEQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 본문의 Full_chunk가 너무 길때"
   ],
   "metadata": {
    "id": "AK752NU_tHjb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ],
   "metadata": {
    "id": "q64Iw5Wbrd94"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "metadata": {
    "id": "1TVbBTmgr_CA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "retriever.add_documents(docs)"
   ],
   "metadata": {
    "id": "K9bPu58hsAfP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(list(store.yield_keys()))"
   ],
   "metadata": {
    "id": "KLvoTdeksUne"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sub_docs = vectorstore.similarity_search(\"인공지능 예산\")"
   ],
   "metadata": {
    "id": "K0Te96C7sVxA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(sub_docs[0].page_content)"
   ],
   "metadata": {
    "id": "qJ6Qb_E_sZCb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(sub_docs[0].page_content)"
   ],
   "metadata": {
    "id": "GXioc_E3so_U"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"인공지능 예산\")"
   ],
   "metadata": {
    "id": "bL6F-aBJsZyv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(retrieved_docs[0].page_content)"
   ],
   "metadata": {
    "id": "EnhD6dY_sfKq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(retrieved_docs[0].page_content)"
   ],
   "metadata": {
    "id": "UhxrZ1zAscVP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self-querying\n",
    "\n",
    "자동 질의 생성(Auto-Query Generation): 시스템이 사용자의 정보 요청을 이해하고, 필요한 정보를 얻기 위해 스스로 질의를 생성합니다.\n",
    "\n",
    "질의-응답 매핑(Query-Response Mapping): 생성된 질의에 대해 적절한 응답을 검색하고, 사용자가 원하는 정보를 제공하는 방식입니다."
   ],
   "metadata": {
    "id": "FoTL9eBctWva"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install lark",
   "metadata": {
    "id": "_DmHA3cgtZcJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ],
   "metadata": {
    "id": "7TJLhLfQthCt"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"sk-\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ],
   "metadata": {
    "id": "9x2PsJ9e7tuH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "retriever.get_relevant_documents(\"what are some movies rated higher than 8.5\")"
   ],
   "metadata": {
    "id": "foYY4IEA74Dr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time-weighted vector store Retriever"
   ],
   "metadata": {
    "id": "zQP9iztA8ApB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scoring 방법 = *semantic_similarity + (1.0 - decay_rate) ^ hours_passed*"
   ],
   "metadata": {
    "id": "OKEps8vK8GIm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q faiss-gpu"
   ],
   "metadata": {
    "id": "zKL8WJ_P8UEG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n"
   ],
   "metadata": {
    "id": "uLG48BJY8AEW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 768\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, decay_rate=0.99, k=1\n",
    ")"
   ],
   "metadata": {
    "id": "Lv5TtUnK8Mml"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents(\n",
    "    [Document(page_content=\"영어는 훌륭합니다.\", metadata={\"last_accessed_at\": yesterday})]\n",
    ")\n",
    "retriever.add_documents([Document(page_content=\"한국어는 훌륭합니다\")])"
   ],
   "metadata": {
    "id": "vL-HP-9L8OIq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"영어가 좋아요\")"
   ],
   "metadata": {
    "id": "Q6KCbXXV_d4F"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "sjtkFgOGfemK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UNvjmQFakb7U"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Retriever"
   ],
   "metadata": {
    "id": "ltRw8HCmmECS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb langchain-openai faiss-gpu --upgrade --quiet  rank_bm25 > /dev/null"
   ],
   "metadata": {
    "id": "QN9Qk5_LkeQH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ],
   "metadata": {
    "id": "h7HV_yPpmKdX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "OYhREH6zmMJp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[복지이슈 FOCUS 15ȣ] 경기도 극저신용대출심사모형 개발을 위한 국내 신용정보 활용가능성 탐색.pdf\"),\n",
    "    PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)"
   ],
   "metadata": {
    "id": "15n17ed7meJI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "embedding = ko_embedding\n",
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ],
   "metadata": {
    "id": "ck2cUaLX4pa3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "docs = ensemble_retriever.invoke(\"혁신정책금융과 극저신용대출모형의 차이\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ],
   "metadata": {
    "id": "JcBSDvy3mmlx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = faiss_retriever.invoke(\"혁신정책금융과 극저신용대출모형의 차이\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ],
   "metadata": {
    "id": "c0Jhox_l6qe2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = ensemble_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"극저신용자 대출의 신용등급\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ],
   "metadata": {
    "id": "28vloMiumuY_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ],
   "metadata": {
    "id": "LxPfaF0L0c2I"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = faiss_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"극저신용자 대출의 신용등급\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ],
   "metadata": {
    "id": "bxftciR52lYW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ],
   "metadata": {
    "id": "kId5kM9H3LxY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MXZ7dGLm3e8A"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Long Context Reorder"
   ],
   "metadata": {
    "id": "-YinIRx6_--y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"보스턴 셀틱스에 관한 문서입니다.\", \"보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"저는 영화 보러 가는 것을 좋아해요\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적 인 NBA 선수였습니다.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=ko_embedding).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱스에 대해 어떤 이야기를 들려주시겠어요?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ],
   "metadata": {
    "id": "53bnfKrCAPxj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ],
   "metadata": {
    "id": "x_9sEUx0AX9h"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm_chain = LLMChain(llm=openai, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "jxdYbGqgA4Sf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
    "result = chain.run(input_documents=docs, query=query)\n",
    "\n",
    "print(reordered_result)\n",
    "print(\"-\"*100)\n",
    "print(result)"
   ],
   "metadata": {
    "id": "Cn0oLiy3D_WR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JaIjA6LWEdP2"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
